{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfc3f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.3000, 2.9000, 5.6000, 1.8000]), tensor([1., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# parameters\n",
    "training_epochs = 100\n",
    "batch_size = 25\n",
    "\n",
    "#데이터 삽입\n",
    "train = np.loadtxt(\"training.dat\")\n",
    "train=torch.FloatTensor(train)\n",
    "\n",
    "train_X = train\n",
    "train_Y = []\n",
    "for i in range(len(train_X)):\n",
    "    if i >= 0 and i < 25:\n",
    "        train_Y.append(0)\n",
    "    elif i >= 25 and i<50:\n",
    "        train_Y.append(1)\n",
    "    else:\n",
    "        train_Y.append(2)\n",
    "train_Y = torch.tensor(train_Y)\n",
    "train_Y = F.one_hot(train_Y)#OneHotEncoding\n",
    "train_Y = train_Y.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "test = np.loadtxt(\"testing.dat\")\n",
    "test = torch.FloatTensor(test)\n",
    "\n",
    "test_x = test\n",
    "test_y = []\n",
    "for i in range(len(test_x)):\n",
    "    if i < 25:\n",
    "        test_y.append(0)\n",
    "    elif i >= 25 and i<50:\n",
    "        test_y.append(1)\n",
    "    else:\n",
    "        test_y.append(2)\n",
    "test_y = torch.tensor(test_y)\n",
    "test_y = F.one_hot(test_y)#OneHotEncoding\n",
    "test_y = test_y.type(torch.FloatTensor)\n",
    "\n",
    "Train_Data=[]\n",
    "for i in range(len(train)):\n",
    "    Train_Data.append((train_X[i],train_Y[i]))\n",
    "\n",
    "print(Train_Data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d27d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=Train_Data,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          drop_last=False)\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): #input: sample의 size  hidden: output의 size\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_layer  = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        output = self.relu(self.input_layer(x))\n",
    "        output = self.relu(self.hidden_layer1(output))\n",
    "        output = self.output_layer(output)\n",
    "        output = self.soft(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b8f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NeuralNet(4,16,16,16,3)#입력 열의 개수, 히든 레이어 노드수(2의 n승, 입력받는 열의 개수 보다 많이)\n",
    "model = NeuralNet(4,18,3)\n",
    "learning_rate=0.01 #학습율 설정(수정해서 돌려보기) 러닝레이트 스케줄러\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=training_epochs/10, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39f352b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 1.128814340 Accuracy: 0.000000000\n",
      "Epoch: 0002 cost = 1.096067190 Accuracy: 33.333333333\n",
      "Epoch: 0003 cost = 1.081895947 Accuracy: 33.333333333\n",
      "Epoch: 0004 cost = 1.070480943 Accuracy: 33.333333333\n",
      "Epoch: 0005 cost = 1.063219666 Accuracy: 33.333333333\n",
      "Epoch: 0006 cost = 1.054602027 Accuracy: 33.333333333\n",
      "Epoch: 0007 cost = 1.045199990 Accuracy: 33.333333333\n",
      "Epoch: 0008 cost = 1.038360953 Accuracy: 33.333333333\n",
      "Epoch: 0009 cost = 1.031660318 Accuracy: 33.333333333\n",
      "Epoch: 0010 cost = 1.022397399 Accuracy: 33.333333333\n",
      "Epoch: 0011 cost = 1.009182096 Accuracy: 64.000000000\n",
      "Epoch: 0012 cost = 0.994051158 Accuracy: 65.333333333\n",
      "Epoch: 0013 cost = 0.976878166 Accuracy: 66.666666667\n",
      "Epoch: 0014 cost = 0.956431270 Accuracy: 66.666666667\n",
      "Epoch: 0015 cost = 0.934233308 Accuracy: 66.666666667\n",
      "Epoch: 0016 cost = 0.910668015 Accuracy: 66.666666667\n",
      "Epoch: 0017 cost = 0.888085186 Accuracy: 66.666666667\n",
      "Epoch: 0018 cost = 0.869318485 Accuracy: 66.666666667\n",
      "Epoch: 0019 cost = 0.855081201 Accuracy: 66.666666667\n",
      "Epoch: 0020 cost = 0.844020069 Accuracy: 66.666666667\n",
      "Epoch: 0021 cost = 0.834830523 Accuracy: 66.666666667\n",
      "Epoch: 0022 cost = 0.826628566 Accuracy: 66.666666667\n",
      "Epoch: 0023 cost = 0.818988562 Accuracy: 66.666666667\n",
      "Epoch: 0024 cost = 0.811558962 Accuracy: 66.666666667\n",
      "Epoch: 0025 cost = 0.804498136 Accuracy: 66.666666667\n",
      "Epoch: 0026 cost = 0.797960997 Accuracy: 66.666666667\n",
      "Epoch: 0027 cost = 0.791890442 Accuracy: 66.666666667\n",
      "Epoch: 0028 cost = 0.786054671 Accuracy: 70.666666667\n",
      "Epoch: 0029 cost = 0.780197799 Accuracy: 78.666666667\n",
      "Epoch: 0030 cost = 0.774116635 Accuracy: 82.666666667\n",
      "Epoch: 0031 cost = 0.767834425 Accuracy: 85.333333333\n",
      "Epoch: 0032 cost = 0.761271298 Accuracy: 88.000000000\n",
      "Epoch: 0033 cost = 0.754413664 Accuracy: 90.666666667\n",
      "Epoch: 0034 cost = 0.747418523 Accuracy: 94.666666667\n",
      "Epoch: 0035 cost = 0.740380347 Accuracy: 94.666666667\n",
      "Epoch: 0036 cost = 0.733249128 Accuracy: 94.666666667\n",
      "Epoch: 0037 cost = 0.726080596 Accuracy: 94.666666667\n",
      "Epoch: 0038 cost = 0.718852997 Accuracy: 96.000000000\n",
      "Epoch: 0039 cost = 0.711605906 Accuracy: 96.000000000\n",
      "Epoch: 0040 cost = 0.704458773 Accuracy: 96.000000000\n",
      "Epoch: 0041 cost = 0.697533846 Accuracy: 96.000000000\n",
      "Epoch: 0042 cost = 0.690891981 Accuracy: 96.000000000\n",
      "Epoch: 0043 cost = 0.684468627 Accuracy: 96.000000000\n",
      "Epoch: 0044 cost = 0.678039014 Accuracy: 96.000000000\n",
      "Epoch: 0045 cost = 0.671924651 Accuracy: 96.000000000\n",
      "Epoch: 0046 cost = 0.666161001 Accuracy: 97.333333333\n",
      "Epoch: 0047 cost = 0.660815001 Accuracy: 97.333333333\n",
      "Epoch: 0048 cost = 0.655850053 Accuracy: 97.333333333\n",
      "Epoch: 0049 cost = 0.651001573 Accuracy: 97.333333333\n",
      "Epoch: 0050 cost = 0.646345854 Accuracy: 97.333333333\n",
      "Epoch: 0051 cost = 0.642150462 Accuracy: 97.333333333\n",
      "Epoch: 0052 cost = 0.638326108 Accuracy: 97.333333333\n",
      "Epoch: 0053 cost = 0.634714007 Accuracy: 97.333333333\n",
      "Epoch: 0054 cost = 0.631215394 Accuracy: 97.333333333\n",
      "Epoch: 0055 cost = 0.627966106 Accuracy: 97.333333333\n",
      "Epoch: 0056 cost = 0.625002623 Accuracy: 97.333333333\n",
      "Epoch: 0057 cost = 0.622265220 Accuracy: 97.333333333\n",
      "Epoch: 0058 cost = 0.619705379 Accuracy: 97.333333333\n",
      "Epoch: 0059 cost = 0.617297590 Accuracy: 97.333333333\n",
      "Epoch: 0060 cost = 0.615045726 Accuracy: 98.666666667\n",
      "Epoch: 0061 cost = 0.612955213 Accuracy: 98.666666667\n",
      "Epoch: 0062 cost = 0.611012638 Accuracy: 98.666666667\n",
      "Epoch: 0063 cost = 0.609193087 Accuracy: 98.666666667\n",
      "Epoch: 0064 cost = 0.607486248 Accuracy: 98.666666667\n",
      "Epoch: 0065 cost = 0.605864406 Accuracy: 98.666666667\n",
      "Epoch: 0066 cost = 0.604347408 Accuracy: 98.666666667\n",
      "Epoch: 0067 cost = 0.602939487 Accuracy: 98.666666667\n",
      "Epoch: 0068 cost = 0.601620257 Accuracy: 98.666666667\n",
      "Epoch: 0069 cost = 0.600366831 Accuracy: 98.666666667\n",
      "Epoch: 0070 cost = 0.599173248 Accuracy: 98.666666667\n",
      "Epoch: 0071 cost = 0.598046362 Accuracy: 98.666666667\n",
      "Epoch: 0072 cost = 0.596987903 Accuracy: 98.666666667\n",
      "Epoch: 0073 cost = 0.595987558 Accuracy: 98.666666667\n",
      "Epoch: 0074 cost = 0.595035732 Accuracy: 98.666666667\n",
      "Epoch: 0075 cost = 0.594127417 Accuracy: 98.666666667\n",
      "Epoch: 0076 cost = 0.593263209 Accuracy: 98.666666667\n",
      "Epoch: 0077 cost = 0.592443049 Accuracy: 98.666666667\n",
      "Epoch: 0078 cost = 0.591662109 Accuracy: 98.666666667\n",
      "Epoch: 0079 cost = 0.590917528 Accuracy: 98.666666667\n",
      "Epoch: 0080 cost = 0.590205431 Accuracy: 98.666666667\n",
      "Epoch: 0081 cost = 0.589525759 Accuracy: 98.666666667\n",
      "Epoch: 0082 cost = 0.588870883 Accuracy: 98.666666667\n",
      "Epoch: 0083 cost = 0.588244796 Accuracy: 98.666666667\n",
      "Epoch: 0084 cost = 0.587649584 Accuracy: 98.666666667\n",
      "Epoch: 0085 cost = 0.587084234 Accuracy: 98.666666667\n",
      "Epoch: 0086 cost = 0.586527705 Accuracy: 98.666666667\n",
      "Epoch: 0087 cost = 0.585991502 Accuracy: 98.666666667\n",
      "Epoch: 0088 cost = 0.585485101 Accuracy: 98.666666667\n",
      "Epoch: 0089 cost = 0.585003793 Accuracy: 98.666666667\n",
      "Epoch: 0090 cost = 0.584537208 Accuracy: 98.666666667\n",
      "Epoch: 0091 cost = 0.584081054 Accuracy: 98.666666667\n",
      "Epoch: 0092 cost = 0.583639383 Accuracy: 98.666666667\n",
      "Epoch: 0093 cost = 0.583216786 Accuracy: 98.666666667\n",
      "Epoch: 0094 cost = 0.582811713 Accuracy: 98.666666667\n",
      "Epoch: 0095 cost = 0.582419217 Accuracy: 98.666666667\n",
      "Epoch: 0096 cost = 0.582036734 Accuracy: 98.666666667\n",
      "Epoch: 0097 cost = 0.581665516 Accuracy: 98.666666667\n",
      "Epoch: 0098 cost = 0.581307411 Accuracy: 98.666666667\n",
      "Epoch: 0099 cost = 0.580961049 Accuracy: 98.666666667\n",
      "Epoch: 0100 cost = 0.580626070 Accuracy: 100.000000000\n"
     ]
    }
   ],
   "source": [
    "dtype=torch.long\n",
    "plt_test=[]\n",
    "train_Sm = torch.nn.Softmax(dim=1)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    count = 0\n",
    "    list_train_x=[]\n",
    "    list_train_y=[]\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X,label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        train_output=model(X)\n",
    "        cost = criterion(train_output, label) #정답::: 0~9 // 원핫인코더는 X\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        arg_train_x = torch.argmax(train_output,1)\n",
    "        arg_train_y = torch.argmax(label,1)\n",
    "   \n",
    "        list_train_x += arg_train_x.tolist()\n",
    "        list_train_y += arg_train_y.tolist()\n",
    "        avg_cost += cost / total_batch \n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost),end=\" \")\n",
    "    for i in range(len(list_train_x)): #예측 값과 정답 값이 같은 것 count하기\n",
    "        if list_train_x[i] == list_train_y[i]:\n",
    "            count+=1\n",
    "    print('Accuracy:','{:.9f}'.format(count/len(train_X)*100))#예측 값과 정답 값이 같은 지를 퍼센테이지로 나타내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4968c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2])\n",
      "test accuracy: 94.666666667\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_count = 0\n",
    "    test_avg_cost = 0\n",
    "    list_test_x=[]\n",
    "    list_test_y=[]\n",
    "    correct = 0\n",
    "        \n",
    "    prediction = model(test_x)\n",
    "    correct_cost = criterion(prediction, test_y)\n",
    "\n",
    "    arg_test_x = torch.argmax(prediction,1)\n",
    "    print(arg_test_x)\n",
    "    arg_test_y = torch.argmax(test_y,1)\n",
    "    print(arg_test_y)\n",
    "\n",
    "    list_test_x += arg_test_x.tolist()\n",
    "    list_test_y += arg_test_y.tolist()\n",
    "    \n",
    "    test_avg_cost += correct_cost / total_batch \n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(list_test_y)): #예측 값과 정답 값이 같은 것 count하기\n",
    "        if list_test_x[i] == list_test_y[i]:\n",
    "            test_count+=1\n",
    " \n",
    "    print('test accuracy:','{:.9f}'.format(test_count/len(test_x)*100))#예측 값과 정답 값이 같은 지를 퍼센테이지로 나타내기 \n",
    "\n",
    "#     correct_test = [list_test_x[i] == list_test_y[i] for i in range(len(list_test_y))]           \n",
    "#     for i in range(len(correct_test)): #예측 값과 정답 값이 같은 것 count하기\n",
    "#         if correct_test[i] == True:\n",
    "#             test_count+=1\n",
    "#     print('Accuracy:','{:.9f}'.format(test_count/len(test_x)*100))#예측 값과 정답 값이 같은 지를 퍼센테이지로 나타내기\n",
    "#     print('result equal: ', [i for i in range(len(correct_test)) if correct_test[i] == False],'\\n')#예측 값과 정답 값이 같은 거 출력    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a12f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
